{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install necessary packages\n",
                "!pip install scikit-learn pandas joblib networkx numpy sqlalchemy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import json\n",
                "import networkx as nx\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.model_selection import train_test_split, cross_val_score\n",
                "from sklearn.metrics import classification_report, accuracy_score\n",
                "import joblib\n",
                "\n",
                "# Add parent directory to path\n",
                "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
                "\n",
                "from backend.storage import load_graph_as_networkx"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Load Graph and Embeddings\n",
                "print(\"Loading graph from database...\")\n",
                "G = load_graph_as_networkx()\n",
                "print(f\"Graph loaded: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Prepare Data\n",
                "data = []\n",
                "\n",
                "# Define heuristics\n",
                "ML_KEYWORDS = {'python', 'jupyter', 'tensorflow', 'pytorch', 'scikit-learn', 'numpy', 'pandas', 'machine-learning', 'deep-learning', 'data-science', 'keras', 'cv', 'nlp', 'r', 'julia'}\n",
                "WEB_KEYWORDS = {'javascript', 'typescript', 'html', 'css', 'react', 'vue', 'angular', 'node', 'django', 'flask', 'web', 'frontend', 'backend', 'php', 'laravel', 'ruby', 'rails', 'java', 'spring'}\n",
                "DEVOPS_KEYWORDS = {'docker', 'kubernetes', 'aws', 'jenkins', 'ansible', 'terraform', 'ci/cd', 'devops', 'bash', 'shell', 'go', 'golang', 'linux', 'cloud', 'c', 'c++', 'rust', 'systems'}\n",
                "\n",
                "def determine_role(languages, tags):\n",
                "    all_terms = [str(t).lower() for t in languages + tags]\n",
                "    ml_score = sum(1 for item in all_terms if item in ML_KEYWORDS)\n",
                "    web_score = sum(1 for item in all_terms if item in WEB_KEYWORDS)\n",
                "    devops_score = sum(1 for item in all_terms if item in DEVOPS_KEYWORDS)\n",
                "    \n",
                "    scores = {'ml': ml_score, 'web': web_score, 'devops': devops_score}\n",
                "    best_role = max(scores, key=scores.get)\n",
                "    \n",
                "    if scores[best_role] == 0:\n",
                "        return 'unknown'\n",
                "    return best_role\n",
                "\n",
                "print(\"Extracting features and labels...\")\n",
                "for node_id, attrs in G.nodes(data=True):\n",
                "    if attrs.get('type') not in ['github_user', 'so_user']:\n",
                "        continue\n",
                "        \n",
                "    embedding = attrs.get('embedding')\n",
                "    if not embedding:\n",
                "        continue\n",
                "        \n",
                "    languages = []\n",
                "    tags = []\n",
                "    \n",
                "    for neighbor in G.neighbors(node_id):\n",
                "        n_attrs = G.nodes[neighbor]\n",
                "        \n",
                "        if n_attrs.get('type') == 'github_repo':\n",
                "            lang = n_attrs.get('language')\n",
                "            if lang:\n",
                "                languages.append(lang)\n",
                "        elif n_attrs.get('type') == 'stackoverflow_tag':\n",
                "            tag_name = n_attrs.get('tag_name')\n",
                "            if tag_name:\n",
                "                tags.append(tag_name)\n",
                "        elif n_attrs.get('type') == 'so_user' and attrs.get('type') == 'github_user':\n",
                "             for so_neighbor in G.neighbors(neighbor):\n",
                "                 so_n_attrs = G.nodes[so_neighbor]\n",
                "                 if so_n_attrs.get('type') == 'stackoverflow_tag':\n",
                "                     tag_name = so_n_attrs.get('tag_name')\n",
                "                     if tag_name:\n",
                "                         tags.append(tag_name)\n",
                "    \n",
                "    role = determine_role(languages, tags)\n",
                "    \n",
                "    if role != 'unknown':\n",
                "        data.append({\n",
                "            'node_id': node_id,\n",
                "            'embedding': embedding,\n",
                "            'role': role,\n",
                "            'languages': languages,\n",
                "            'tags': tags\n",
                "        })\n",
                "\n",
                "df = pd.DataFrame(data)\n",
                "print(f\"Found {len(df)} labeled users.\")\n",
                "if len(df) > 0:\n",
                "    print(df['role'].value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Train Classifier\n",
                "if len(df) < 2 or len(df['role'].unique()) < 2:\n",
                "    print(\"Not enough data or classes to train. Please fetch more diverse data.\")\n",
                "else:\n",
                "    X = np.array(df['embedding'].tolist())\n",
                "    y = df['role']\n",
                "    \n",
                "    print(f\"Training on {len(X)} samples...\")\n",
                "    try:\n",
                "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
                "    except ValueError:\n",
                "        X_train, y_train = X, y\n",
                "        X_test, y_test = [], []\n",
                "    \n",
                "    clf = LogisticRegression(max_iter=1000, multi_class='ovr')\n",
                "    clf.fit(X_train, y_train)\n",
                "    \n",
                "    # 4. Evaluation\n",
                "    if len(X_test) > 0:\n",
                "        y_pred = clf.predict(X_test)\n",
                "        print(\"\\nClassification Report:\")\n",
                "        print(classification_report(y_test, y_pred))\n",
                "    \n",
                "    # 5. Save Model\n",
                "    model_dir = os.path.join('..', 'backend', 'models')\n",
                "    os.makedirs(model_dir, exist_ok=True)\n",
                "    model_path = os.path.join(model_dir, 'role_clf.joblib')\n",
                "    joblib.dump(clf, model_path)\n",
                "    print(f\"Model saved to {model_path}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Sample Prediction\n",
                "if len(df) > 0 and 'clf' in locals():\n",
                "    print(\"\\nSample Prediction:\")\n",
                "    sample_idx = 0\n",
                "    sample_user = df.iloc[sample_idx]\n",
                "    \n",
                "    pred_role = clf.predict([sample_user['embedding']])[0]\n",
                "    probs = clf.predict_proba([sample_user['embedding']])[0]\n",
                "    \n",
                "    print(f\"User: {sample_user['node_id']}\")\n",
                "    print(f\"True Role: {sample_user['role']}\")\n",
                "    print(f\"Predicted: {pred_role}\")\n",
                "    print(f\"Probabilities: {dict(zip(clf.classes_, probs))}\")\n",
                "    print(f\"Languages/Tags: {sample_user['languages'] + sample_user['tags']}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}